{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "431d5116",
   "metadata": {},
   "source": [
    "# Complete In-Depth Explanation: Spark Structured Streaming with Auto Loader\n",
    "\n",
    "Let me break down this code comprehensively so you can understand every concept and implement it confidently in real projects.\n",
    "\n",
    "## **PART 1: Reading Stream with Auto Loader (Cloud Files)**\n",
    "\n",
    "```python\n",
    "df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/Volumes/anuj_catalog/bronze/autoload/destination/checkpoint/\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "    .load(\"/Volumes/anuj_catalog/bronze/autoload/raw/\"))\n",
    "```\n",
    "\n",
    "### **1.1 `spark.readStream`**\n",
    "- Creates a **streaming DataFrame** (not a regular DataFrame)\n",
    "- Continuously monitors the source location for new files\n",
    "- Unlike `spark.read` (batch), this processes data incrementally as it arrives\n",
    "\n",
    "### **1.2 `.format(\"cloudFiles\")`**\n",
    "- **Auto Loader** - Databricks' optimized way to ingest files from cloud storage\n",
    "- **Why use it?**\n",
    "  - Automatically tracks which files have been processed\n",
    "  - More efficient than directory listing (uses file notification queues)\n",
    "  - Handles schema inference and evolution automatically\n",
    "  - Scalable to millions of files\n",
    "\n",
    "**Alternative formats:**\n",
    "- `\"delta\"` - Read from Delta tables\n",
    "- `\"kafka\"` - Read from Kafka streams\n",
    "- `\"socket\"` - Read from TCP socket\n",
    "- `\"rate\"` - Generate test data\n",
    "\n",
    "### **1.3 `.option(\"cloudFiles.format\", \"csv\")`**\n",
    "- Specifies the **file format** of incoming files\n",
    "- Auto Loader will parse files as CSV\n",
    "\n",
    "**Other supported formats:**\n",
    "- `\"json\"`, `\"parquet\"`, `\"avro\"`, `\"orc\"`, `\"text\"`, `\"binaryFile\"`\n",
    "\n",
    "### **1.4 `.option(\"cloudFiles.schemaLocation\", \"/path/\")`**\n",
    "- **Critical for production!**\n",
    "- Stores the **inferred schema** in this location\n",
    "- On first run: Auto Loader samples files and saves schema here\n",
    "- On subsequent runs: Uses the saved schema (ensures consistency)\n",
    "- **Without this:** Schema inference happens every time (slow & risky)\n",
    "\n",
    "**Schema Location contains:**\n",
    "- `_schemas/` folder with schema metadata\n",
    "- Versioned schema files for evolution tracking\n",
    "\n",
    "### **1.5 `.option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")`**\n",
    "- Handles **schema changes** in incoming files\n",
    "\n",
    "**Three modes:**\n",
    "\n",
    "| Mode | Behavior | Use Case |\n",
    "|------|----------|----------|\n",
    "| `\"addOnly\"` | Allows adding new columns only | Controlled evolution |\n",
    "| `\"rescue\"` | Saves unparsable/new data in `_rescued_data` column | Never lose data, handle later |\n",
    "| `\"failOnNewColumns\"` | Fails stream if schema changes | Strict schemas |\n",
    "| `\"none\"` (default) | Ignores schema changes | Static schemas |\n",
    "\n",
    "**How `rescue` works:**\n",
    "```\n",
    "Original schema: id, name, age\n",
    "New file adds: id, name, age, email, phone\n",
    "\n",
    "Result columns: id, name, age, _rescued_data\n",
    "_rescued_data contains: {\"email\": \"...\", \"phone\": \"...\"}\n",
    "```\n",
    "\n",
    "### **1.6 `.load(\"/Volumes/anuj_catalog/bronze/autoload/raw/\")`**\n",
    "- **Source directory** where files arrive\n",
    "- Auto Loader monitors this continuously\n",
    "- Processes files as they land (incremental)\n",
    "\n",
    "---\n",
    "\n",
    "## **PART 2: Writing Stream to Delta Lake**\n",
    "\n",
    "```python\n",
    "df.writeStream.format(\"delta\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"checkpointLocation\", \"/path/\")\\\n",
    "    .trigger(availableNow=True)\\\n",
    "    .start(\"/Volumes/anuj_catalog/bronze/autoload/destination/data\")\n",
    "```\n",
    "\n",
    "### **2.1 `.writeStream`**\n",
    "- Initiates streaming write operation\n",
    "- Returns a `DataStreamWriter` object\n",
    "\n",
    "### **2.2 `.format(\"delta\")`**\n",
    "- Writes to **Delta Lake format**\n",
    "- Why Delta?\n",
    "  - ACID transactions\n",
    "  - Time travel\n",
    "  - Schema enforcement & evolution\n",
    "  - Efficient upserts/merges\n",
    "  - Better performance than Parquet for streaming\n",
    "\n",
    "**Other formats:**\n",
    "- `\"parquet\"`, `\"orc\"`, `\"json\"`, `\"csv\"` (less common for streaming)\n",
    "- `\"console\"` (debugging - prints to console)\n",
    "- `\"memory\"` (debugging - stores in memory table)\n",
    "\n",
    "### **2.3 `.outputMode(\"append\")`**\n",
    "- How to write data to the sink\n",
    "\n",
    "**Three output modes:**\n",
    "\n",
    "| Mode | Description | Use Case | Restrictions |\n",
    "|------|-------------|----------|--------------|\n",
    "| `\"append\"` | Only new rows added | Most common, event logs, IoT data | Can't use with aggregations without watermark |\n",
    "| `\"complete\"` | Entire result rewritten each time | Small aggregated tables, dashboards | Only for aggregations, high cost |\n",
    "| `\"update\"` | Only changed rows updated | Aggregations with watermark | Only for aggregations |\n",
    "\n",
    "### **2.4 `.option(\"checkpointLocation\", \"/path/\")`**\n",
    "- **MOST IMPORTANT for production streaming!**\n",
    "- Stores **stream state** and processing metadata\n",
    "\n",
    "**What's stored in checkpoint:**\n",
    "- Offsets of processed files/records\n",
    "- Stream query metadata\n",
    "- State information for stateful operations\n",
    "- Allows **exactly-once processing** guarantees\n",
    "\n",
    "**Critical rules:**\n",
    "- Never delete while stream is running\n",
    "- Don't share checkpoints between different streams\n",
    "- Use different checkpoints for read vs write operations\n",
    "\n",
    "**In your code:** You're using **same location for schema and checkpoint** - this is fine but better to separate:\n",
    "```python\n",
    "# Better practice:\n",
    "checkpointLocation = \"/Volumes/.../checkpoints/stream1/\"\n",
    "schemaLocation = \"/Volumes/.../schemas/stream1/\"\n",
    "```\n",
    "\n",
    "### **2.5 `.trigger(availableNow=True)`**\n",
    "- Controls **when micro-batches run**\n",
    "\n",
    "**Trigger types:**\n",
    "\n",
    "| Trigger | Behavior | Use Case |\n",
    "|---------|----------|----------|\n",
    "| `availableNow=True` | Process all available data once, then stop | Near real-time batch (like scheduled jobs) |\n",
    "| `once=True` | Process one micro-batch, then stop | Manual/cron-triggered |\n",
    "| `processingTime=\"5 seconds\"` | Run every 5 seconds continuously | True streaming, low latency |\n",
    "| `continuous=\"1 second\"` | Experimental continuous processing | Ultra-low latency (milliseconds) |\n",
    "| Default (no trigger) | Run micro-batch as soon as previous completes | Maximum throughput |\n",
    "\n",
    "**Your `availableNow=True`:**\n",
    "- Perfect for **incremental batch processing**\n",
    "- Processes everything new since last run\n",
    "- Stops automatically when done\n",
    "- Can schedule with Databricks Jobs\n",
    "- More cost-effective than 24/7 streaming\n",
    "\n",
    "### **2.6 `.start(\"/path/\")`**\n",
    "- **Starts the streaming query**\n",
    "- Path is the **destination** for Delta table\n",
    "- Returns `StreamingQuery` object for monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## **PART 3: Additional Important Options & Concepts**\n",
    "\n",
    "### **3.1 More Auto Loader Options**\n",
    "\n",
    "```python\n",
    "df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    \n",
    "    # Schema options\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/path/schema/\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")  # Infer int, double, etc.\n",
    "    .option(\"cloudFiles.schemaHints\", \"id INT, created_date DATE\")  # Force types\n",
    "    \n",
    "    # File notification vs directory listing\n",
    "    .option(\"cloudFiles.useNotifications\", \"true\")  # Use queue notifications (faster)\n",
    "    .option(\"cloudFiles.includeExistingFiles\", \"true\")  # Process existing files on first run\n",
    "    \n",
    "    # Performance\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\", \"1000\")  # Rate limiting\n",
    "    .option(\"cloudFiles.maxBytesPerTrigger\", \"10g\")  # Control batch size\n",
    "    \n",
    "    # File handling\n",
    "    .option(\"cloudFiles.validateOptions\", \"true\")  # Validate on startup\n",
    "    .option(\"pathGlobFilter\", \"*.csv\")  # Only process specific files\n",
    "    \n",
    "    # CSV specific\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"inferSchema\", \"true\")  # For non-cloudFiles format\n",
    "    .option(\"mode\", \"PERMISSIVE\")  # DROPMALFORMED, FAILFAST\n",
    "    \n",
    "    .load(\"/path/\"))\n",
    "```\n",
    "\n",
    "### **3.2 More Write Stream Options**\n",
    "\n",
    "```python\n",
    "(df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    \n",
    "    # Checkpoint\n",
    "    .option(\"checkpointLocation\", \"/path/checkpoint/\")\n",
    "    \n",
    "    # Partitioning\n",
    "    .partitionBy(\"date\", \"region\")  # Partition Delta table\n",
    "    \n",
    "    # Optimization\n",
    "    .option(\"optimizeWrite\", \"true\")  # Auto-optimize file sizes\n",
    "    .option(\"autoCompact\", \"true\")  # Auto-compact small files\n",
    "    \n",
    "    # Idempotency\n",
    "    .option(\"txnAppId\", \"unique-app-id\")  # Idempotent writes\n",
    "    .option(\"txnVersion\", \"1\")\n",
    "    \n",
    "    # Merge schema\n",
    "    .option(\"mergeSchema\", \"true\")  # Allow schema evolution on write\n",
    "    \n",
    "    # Triggers\n",
    "    .trigger(availableNow=True)\n",
    "    # .trigger(once=True)\n",
    "    # .trigger(processingTime=\"30 seconds\")\n",
    "    # .trigger(continuous=\"1 second\")\n",
    "    \n",
    "    # Output settings\n",
    "    .queryName(\"my_streaming_query\")  # Name for monitoring\n",
    "    \n",
    "    .start(\"/path/destination/\"))\n",
    "```\n",
    "\n",
    "### **3.3 Monitoring Your Stream**\n",
    "\n",
    "```python\n",
    "# Start stream and capture query object\n",
    "query = (df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/checkpoint/\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start(\"/destination/\"))\n",
    "\n",
    "# Monitor\n",
    "query.id  # Unique query ID\n",
    "query.name  # Query name if set\n",
    "query.status  # Current status\n",
    "query.lastProgress  # Last micro-batch info\n",
    "query.recentProgress  # Recent batches\n",
    "\n",
    "# Wait for completion (availableNow)\n",
    "query.awaitTermination()\n",
    "\n",
    "# Stop (for continuous streams)\n",
    "query.stop()\n",
    "```\n",
    "\n",
    "### **3.4 Error Handling & Recovery**\n",
    "\n",
    "```python\n",
    "# Handle bad records\n",
    "df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"mode\", \"PERMISSIVE\")  # null for bad records\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")  # Save bad rows\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")  # Extra safety\n",
    "    .load(\"/path/\"))\n",
    "\n",
    "# Filter out corrupt records before writing\n",
    "clean_df = df.filter(col(\"_corrupt_record\").isNull())\n",
    "\n",
    "# Separate bad records\n",
    "bad_df = df.filter(col(\"_corrupt_record\").isNotNull())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **PART 4: Complete Real-World Example**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# READ: Auto Loader with all best practices\n",
    "df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/Volumes/catalog/schema/bronze_orders/\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\", \"100\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"mode\", \"PERMISSIVE\")\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "    .load(\"/Volumes/catalog/raw/orders/\"))\n",
    "\n",
    "# TRANSFORM: Add metadata\n",
    "transformed_df = (df\n",
    "    .withColumn(\"ingestion_time\", current_timestamp())\n",
    "    .withColumn(\"file_name\", input_file_name())\n",
    "    .withColumn(\"is_corrupt\", col(\"_corrupt_record\").isNotNull())\n",
    "    .filter(col(\"_corrupt_record\").isNull())  # Only clean records\n",
    "    .drop(\"_corrupt_record\"))\n",
    "\n",
    "# WRITE: Delta with optimization\n",
    "query = (transformed_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/Volumes/catalog/checkpoint/bronze_orders/\")\n",
    "    .option(\"optimizeWrite\", \"true\")\n",
    "    .option(\"autoCompact\", \"true\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .partitionBy(\"ingestion_date\")\n",
    "    .trigger(availableNow=True)\n",
    "    .queryName(\"bronze_orders_ingestion\")\n",
    "    .start(\"/Volumes/catalog/bronze/orders/\"))\n",
    "\n",
    "# Monitor\n",
    "query.awaitTermination()\n",
    "print(f\"Processed: {query.lastProgress}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **PART 5: Key Differences - Batch vs Streaming**\n",
    "\n",
    "| Aspect | Batch (`spark.read`) | Streaming (`spark.readStream`) |\n",
    "|--------|---------------------|--------------------------------|\n",
    "| Data | Static snapshot | Continuous/incremental |\n",
    "| Execution | Runs once | Runs continuously/triggered |\n",
    "| Checkpoint | Not needed | Required for fault tolerance |\n",
    "| Schema | Can change freely | Needs schema location |\n",
    "| Operations | All supported | Limited (no sorting without window) |\n",
    "| Cost | Per execution | Continuous cost (unless availableNow) |\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways for Production:**\n",
    "\n",
    "1. **Always use checkpoint locations** (different for read & write)\n",
    "2. **Use `availableNow=True`** for cost-effective near-real-time\n",
    "3. **Schema location** prevents inference overhead\n",
    "4. **`rescue` mode** ensures no data loss during schema changes\n",
    "5. **Partition your Delta tables** for query performance\n",
    "6. **Monitor with query.lastProgress** for debugging\n",
    "7. **Separate bronze/silver/gold layers** in your architecture\n",
    "\n",
    "This setup is perfect for a **medallion architecture** where you incrementally ingest raw data into bronze, then transform to silver/gold layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d28515",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
